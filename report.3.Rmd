---
title: "Mission Control Explorations"
author: "Saptarshi Guha"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
---


<style>
body {
    line-height: 1.4em;
    }>
.break-out {
    text-align: center;
    width: 100vw;
    position: relative;
    left: calc(-1 * (100vw - 100%)/2);
}
.r {
    background-color: white;
    border: 0;
        }

pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

</style>

```{r}
library(parallel)
library(brms)
library(glue)
library(data.table)
library(rjson)
library(future)
library(kableExtra)
library(reticulate)
plan(multicore)
slack.start() ##requires prefix.R
knitr::opts_chunk$set(eval=TRUE)
get.dataq=TRUE

```

```{r eval=TRUE,echo=TRUE}
 
qr <- "
with
SAMPLING as (
 SELECT 'nightly' AS chan, 'Linux' as sos, 1 as NBUCKS
 UNION ALL
 SELECT 'nightly' AS chan, 'Darwin' as sos, 1 as NBUCKS
 UNION ALL
 SELECT 'nightly' AS chan, 'Windows_NT' as sos, 1 as NBUCKS
 UNION ALL
 SELECT 'beta' AS chan, 'Linux' as sos, 1 as NBUCKS
 UNION ALL
 SELECT 'beta' AS chan, 'Darwin' as sos, 1 as NBUCKS
 UNION ALL
 SELECT 'beta' AS chan, 'Windows_NT' as sos, 1 as NBUCKS
 UNION ALL
  SELECT 'release' AS chan, 'Linux' as sos, 1 as NBUCKS
 UNION ALL
 SELECT 'release' AS chan, 'Darwin' as sos, 1 as NBUCKS
 UNION ALL
 SELECT 'release' AS chan, 'Windows_NT' as sos, 3 as NBUCKS
),
a1 as (select
--- TOTAL USAGE ON FIREFOX
submission_date_s3 as date,
os,
sum(active_hours_sum) as usage_all,
count(distinct(client_id)) as dau_all
from telemetry.clients_daily_v6  HH left join SAMPLING
on HH.os = SAMPLING.sos and HH.normalized_channel = SAMPLING.chan
where
submission_date_s3 >='{current_version_release}'
and submission_date_s3 <= DATE_ADD(DATE '{current_version_release}', INTERVAL {nday} DAY)
and os  in ('Linux','Windows_NT','Darwin')
and app_name='Firefox'
and normalized_channel = '{norm_channel}'
and  MOD(ABS(FARM_FINGERPRINT(MD5(client_id))), SAMPLING.NBUCKS)=0
group by 1,2
),
a2 as (
--- TOTAL USAGE ON FIREFOX ON LATEST VERSION
--- THIS AND THE ABOVE ARE USED FOR COMPUTING 'NVC'
select
submission_date_s3 as date,
os,
sum(active_hours_sum) as usage_cversion,
count(distinct(client_id)) as dau_cversion
from telemetry.clients_daily_v6 HH left join SAMPLING
on HH.os = SAMPLING.sos and HH.normalized_channel = SAMPLING.chan
where
submission_date_s3 >='{current_version_release}'
and submission_date_s3 <= DATE_ADD(DATE '{current_version_release}', INTERVAL {nday} DAY)
and os  in ('Linux','Windows_NT','Darwin')
and app_name='Firefox'
and normalized_channel = '{norm_channel}'
-- and profile_creation_date>=12418 and profile_creation_date<=20089 
and {app_version_field}='{current_version}'
and  MOD(ABS(FARM_FINGERPRINT(MD5(client_id))), SAMPLING.NBUCKS)=0
group by 1,2
),
A as (
select a1.date,a1.os,a1.usage_all, a1.dau_all, a2.usage_cversion, a2.dau_cversion
from a1 join a2
on a1.date =a2.date and a1.os=a2.os
),
b1 as (
--- Total Crashes on Current Version
--- NEED CLIENT_ID TO JOIN on DAILY TO GET CRASH RATE
select 
client_id,
submission_date as date,
os_name as os, 
sum(case when payload.processType IS NULL OR payload.processType = 'main' then 1 else 0 end) as cmain,
sum(case when payload.processType = 'content' and (udf.get_key(payload.metadata, 'ipc_channel_error') is null
       or (udf.get_key(payload.metadata, 'ipc_channel_error') is not null  and udf.get_key(payload.metadata, 'ipc_channel_error') !='ShutdownKill'))
     then 1 else 0 end) as ccontent
from {crash_src} JJ left join SAMPLING
on JJ.os_name = SAMPLING.sos and JJ.normalized_channel = SAMPLING.chan
where submission_date >='{current_version_release}'
and submission_date <= DATE_ADD(DATE '{current_version_release}', INTERVAL {nday} DAY)
and os_name in ('Linux','Windows_NT','Darwin')
and application='Firefox'
and normalized_channel = '{norm_channel}'
and {build_version_field} in ({current_version_crash})
and profile_created>=12418 and profile_created<=20089 
and  MOD(ABS(FARM_FINGERPRINT(MD5(client_id))), SAMPLING.NBUCKS)=0
group by 1,2,3
),
--- TOTAL HOURS FROM FOLKS WHO CRASHED
--- TO COMPUTE CRASH RATE 
b2 as (select
client_id,
submission_date_s3 as date,
os as os,
sum(active_hours_sum) as usage,
sum(coalesce(crashes_detected_plugin_sum,0)) as cplugin
from telemetry.clients_daily_v6 HH left join SAMPLING
on HH.os = SAMPLING.sos and HH.normalized_channel = SAMPLING.chan
where
submission_date_s3 >='{current_version_release}'
and submission_date_s3 <= DATE_ADD(DATE '{current_version_release}', INTERVAL {nday} DAY)
and os  in ('Linux','Windows_NT','Darwin')
and app_name='Firefox'
and normalized_channel = '{norm_channel}'
and {app_version_field}='{current_version}'
-- and profile_creation_date>=12418 and profile_creation_date<=20089 
and  MOD(ABS(FARM_FINGERPRINT(MD5(client_id))), SAMPLING.NBUCKS)=0
group by 1,2,3
),
b as (
select 
b1.date,b1.os, 
count(distinct(case when b1.cmain>0 then b1.client_id else null end)) as dau_cm_crasher_cversion,
count(distinct(case when b1.ccontent>0  then b1.client_id else null end)) as dau_cc_crasher_cversion,
count(distinct(case when b2.cplugin>0 then b1.client_id else null end)) as dau_cp_crasher_cversion,
count(distinct(case when (b1.cmain>0 or b1.ccontent>0  or b2.cplugin>0)
                        then b1.client_id else null end)) as dau_call_crasher_cversion,
sum(case when b1.cmain>0 then b2.usage else 0 end) as usage_cm_crasher_cversion,
sum(case when b1.ccontent >0  then b2.usage else 0 end) as usage_cc_crasher_cversion,
sum(case when b2.cplugin >0 then b2.usage else 0 end) as usage_cp_crasher_cversion,
sum(case when (b1.cmain>0 or b1.ccontent>0 or b2.cplugin >0) then b2.usage else 0 end) as usage_call_crasher_cversion,
sum(cmain) as cmain,
sum(ccontent)  as ccontent,
sum(cplugin) as cplugin,
sum(cmain)+sum(ccontent) + sum(cplugin) as call
from b1 join b2 
on b1.client_id = b2.client_id and b1.os=b2.os and b1.date = b2.date
where (b1.ccontent+b1.cmain)< 350 -- see https://sql.telemetry.mozilla.org/queries/64354/source
group by 1,2
),
d as (
select
A.date,A.os,A.usage_all, A.dau_all, A.usage_cversion, A.dau_cversion,
b.dau_cm_crasher_cversion,b.dau_cc_crasher_cversion,b.dau_cp_crasher_cversion,b.dau_call_crasher_cversion,
b.usage_cm_crasher_cversion,b.usage_cc_crasher_cversion,b.usage_cp_crasher_cversion,b.usage_call_crasher_cversion,
b.cmain,b.ccontent,b.cplugin,b.call
from b join A
on b.date=A.date and b.os=A.os
)
select * from d order by os, date
"
```

## RELEASE DATA

```{r eval=get.dataq}
library(glue) 
library(data.table)
isn <- function(s,r = 'telemetry.crash_summary_v1') if(is.null(s) || length(s)==0) r else s

fxv <- local({
  r <- fromJSON(file="https://product-details.mozilla.org/1.0/firefox.json")
  rbindlist(lapply(r$releases,function(s){
    data.table(build.number=s$build_number, categ=s$category, date=as.Date(s$date),
               description = isn(s$description,NA), is.security = s$is_security_driven,
               product = s$product,version=s$version)
  }))
})

fxv.releases <- fxv[grepl("(major|stability)",categ),][date >='2019-01-01',]
fxv.releases <- fxv.releases[, ":="(major = as.numeric(sapply(version,function(s) head(strsplit(s,"\\.")[[1]],1))),
                                    minor = as.numeric(sapply(version,function(s) tail(strsplit(s,"\\.")[[1]],1))))]
fxv.releases <- fxv.releases[order(major,minor),]
release.the.current.release <- tail(fxv.releases,1)[, list(version, major,minor, date)]
release.releases.for.model <- fxv.releases[major %in% c(release.the.current.release$major,release.the.current.release$major-1,release.the.current.release$major-2),]


release.what.i.need <- fxv.releases[major %in%  release.the.current.release$major,]
release.what.i.need <- release.what.i.need[,list(v=version,d=date, till=date, c='release',ndays=72, crash_src='telemetry.crash_summary_v2',
                                                 app_version_field='app_version', build_version_field='build_version')]
release.what.i.need <- release.what.i.need[d<'2019-06-18', crash_src := 'telemetry.crash_summary_v1']
release.what.i.need$till= c(tail(release.what.i.need$d,-1),Sys.Date()+365)
g <- bq()

```


```{r getData1,eval=get.dataq,results='hide',message=FALSE, warning=FALSE}
add_fields <- function(x, current_version, d,till ){
  x <- x[,":="(date=as.Date(date))]
  x[,":="(date=as.Date(date),c_version = current_version, c_version_rel = as.Date(d), isLatest=date<=till,t =as.numeric(date - as.Date(d)))][,]
  x<- x[, ":="(
    cmi=  (1+dau_cm_crasher_cversion)/dau_cversion,
    cmr = (1+cmain) / sapply(usage_cm_crasher_cversion,function(s) {if( s< 60/3600){ 0 } else{ s}}),
    cci = (1+dau_cc_crasher_cversion)/dau_cversion,
    ccr = (1+ccontent)/sapply(usage_cc_crasher_cversion,function(s) {if( s< 60/3600){ 0 } else{ s}}),
    nvc=usage_cversion/usage_all,
    os=factor(os, levels=c("Linux","Darwin","Windows_NT")),
    c_version_rel = as.Date(c_version_rel)
  )]
  x
}


######################################################################
## GET NEW DATA FOR CURRENT MAJOR FOR RELEASE
######################################################################

dall.release.new <-   release.what.i.need[,{
    print(.SD);print(.BY)
    qf <- glue(qr,
               current_version=.BY$v,
               current_version_crash=sprintf("'%s'",.BY$v),
               current_version_release=d,
               norm_channel = c,
               app_version_field = app_version_field,
               build_version_field=build_version_field,
               nday=ndays,
               crash_src=isn(crash_src),
               NBUCKS=1
               )
    writeLines(qf,"/tmp/t.txt")
    qff <- g$q(qf,-1)
    if(nrow(qff) > 0)
      add_fields(qff,.BY$v,d,till)
},by=v]
if(nrow(dall.release.new)==0) stop("release: No rows for this data")
dall.release.new <- dall.release.new[!c_version %in% c('66.0.4','66.0.5'),][, {
  ## Take the last day we saw the max
  index <- which.max(nvc)
  s <- (1:.N)[ isLatest]
  if(length(s) > 0){
   whenStoppedBeingLatest <- max( s )
  } else whenStoppedBeingLatest <- -1
  if(index<whenStoppedBeingLatest) index <- whenStoppedBeingLatest
  # .SD[1:whenStoppedBeingLatest,]
  .SD[1:index,]
},by=list(os,c_version)]

dall.release.new <- dall.release.new[, ":="( major = as.numeric(sapply(c_version,function(s) head(strsplit(s,"\\.")[[1]],1))),
                                            minor = as.numeric((sapply(c_version,function(s) tail(strsplit(s,"\\.")[[1]],1)))))]

```

## BETA: DATA

FOR BETA NOW: GET BUILD MAPPING WHICH WILL GET REPLACED BY PYTHON CODE

```{r getBetaData,eval=get.dataq}

buildhub <- import("buildhub_bid")
builds <- local({
 FIX <- function(f)
   paste(unlist(lapply(f,function(s) sprintf("'%s'",s))),collapse=',')
  x <-  buildhub$pull_build_id_docs()
  x <- buildhub$version2build_ids(x)
  x=rbindlist(Map(function(n,b){
    data.table(versions=n, buildid=FIX(lapply(b,"[[",2)))
  },names(x),x))[order(versions),]
})
  

######################################################################
## GET OLD DATA FROM BQ
## NEEDED FOR MODEL
######################################################################

fxv.beta <- fxv[grepl("(dev)",categ),][date >='2019-01-01',]
fxv.beta <- fxv.beta[, ":="( major=  as.numeric(sapply(version,function(s) head(strsplit(s,"\\.0b")[[1]],1))),
                            minor = as.numeric(sapply(version,function(s) tail(strsplit(s,"\\.0b")[[1]],1))))]
fxv.beta <- fxv.beta[order(major,minor),]
beta.the.current.release <- tail(fxv.beta,1)[, list(version, major,minor, date)]
#beta.the.current.release  <- fxv.beta[60,][, list(version, major,minor, date)]
beta.releases.for.model <- fxv.beta[major %in% c(beta.the.current.release$major,beta.the.current.release$major-1,beta.the.current.release$major-2
                                                 ),]

beta.what.i.need <- fxv.beta[major %in%  beta.the.current.release$major,]
beta.what.i.need <- beta.what.i.need[,list(v=version,d=date, till=date, c='beta',ndays=14, crash_src='telemetry.crash_summary_v2',
                                                                         app_version_field='app_display_version', build_version_field='build_id')]
beta.what.i.need$till= c(tail(beta.what.i.need$d,-1),Sys.Date()+7)
beta.what.i.need[v=='69.0b16', till := as.Date('2019-08-27')] ## I need this because we dont get data for RC builds
beta.what.i.need <- beta.what.i.need[d<'2019-06-18', crash_src := 'telemetry.crash_summary_v1']
beta.what.i.need <- merge(beta.what.i.need,builds[, list(v=versions, buildid)],by=c('v'),all.x=TRUE)

```

```{r betadata, eval=get.dataq}
dall.beta.new <-   beta.what.i.need[,{
    print(.SD);print(.BY)
    qf <- glue(qr,
               current_version=.BY$v,
               current_version_crash=buildid,
               current_version_release=d,
               norm_channel = c,
               app_version_field = app_version_field,
               build_version_field=build_version_field,
               nday=ndays,
               crash_src=isn(crash_src),
               NBUCKS=1
               )
    writeLines(qf,'/tmp/x.txt') 
    qff <- g$q(qf,-1)
    if(nrow(qff) > 0)
      add_fields(qff,.BY$v,d,till)
},by=v]
if(nrow(dall.beta.new)==0) stop("beta:No rows for this data")

dall.beta.new <- dall.beta.new[!c_version %in% c('67.0b17','67.0b18'),][, {
  ## Take the last day we saw the max
  index <- which.max(nvc)
  s <- (1:.N)[ isLatest]
  if(length(s) > 0){
   whenStoppedBeingLatest <- max( s )
  } else whenStoppedBeingLatest <- -1
#  if(index<whenStoppedBeingLatest) index <- whenStoppedBeingLatest
  .SD[1:whenStoppedBeingLatest,]
  .SD[1:index,]
},by=list(os,c_version)]
dall.beta.new <- dall.beta.new[,":="( major=  as.numeric(sapply(c_version,function(s) head(strsplit(s,"\\.0b")[[1]],1))),
                                      minor = as.numeric(sapply(c_version,function(s) tail(strsplit(s,"\\.0b")[[1]],1))))]

#dall.new <- rbind(dall.beta.new[,":="(channel='beta')],dall.release.new[,":="(channel='release')])[,":="(v=NULL)]

```


## NIGHTLY

Nightly is slightly different. We get the dates of a version and then each
YYYYMMDD is a minor version. For each minor version we get 7 days of data (we'll
lop of days later).

```{r nightlyData,eval=get.dataq}
## This ideally ought be done via buildhub
nightly <- fxv.beta[minor==3,list(version=substr(version,1,4), date)]
nightly[, ":="(till=c(tail(date,-1),Sys.Date()),
               version = paste(as.numeric(version) +1,".0a1",sep=""))] #HACK
nightly.what.i.need <- tail(nightly,1) #tail(nightly,1)
nightly.what.i.need <- nightly.what.i.need[, data.frame(buildid = strftime(seq(from=date, to=till,by=1),'%Y%m%d')),
                    by=list(v=version)]
nightly.what.i.need[, ":="(d=as.character(strptime(buildid,'%Y%m%d')),
                           c = 'nightly',
                           ndays = 7, crash_src = 'telemetry.crash_summary_v2',
                           app_version_field='substr(app_build_id,1,8)',
                           build_version_field='substr(build_id,1,8)')][, till:=as.Date(d)+7]
nightly.what.i.need <- nightly.what.i.need[d<'2019-06-18', crash_src := 'telemetry.crash_summary_v1']
nightly.releases.for.model <- c(as.numeric(substring(nightly.what.i.need$v,1,2)))
nightly.releases.for.model <- unique(c(nightly.releases.for.model,nightly.releases.for.model-1))


dall.nightly.new <-   nightly.what.i.need[,{
  print(.SD);print(.BY)
    qf <- glue(qr,
               current_version=.BY$buildid,
               current_version_crash=sprintf("'%s'",.BY$buildid),
               current_version_release=d,
               norm_channel = c,
               app_version_field = app_version_field,
               build_version_field=build_version_field,
               nday=ndays,
               crash_src=isn(crash_src),
               NBUCKS=1
               )
  #writeLines(qf, "/tmp/xx")
    qff <- g$q(qf,-1)
    if(nrow(qff) > 0)
      add_fields(qff,.BY$buildid,d,till)
},by=list(v,buildid)]

if(nrow(dall.nightly.new)==0) stop("No rows for this data")

dall.nightly.new <- dall.nightly.new[, {
  ## Take the last day we saw the max
  index <- which.max(nvc)
  .SD[1:index,]
},by=list(os,c_version)]

dall.nightly.new <- dall.nightly.new[,":="( major=  as.numeric(sapply(v,function(s) head(strsplit(s,"\\.0a")[[1]],1))),
                                           minor =  as.numeric(c_version)
                                           ),by=v][,buildid:=NULL]

dall.new <- rbind(dall.beta.new[,":="(channel='beta')],
                  dall.release.new[,":="(channel='release')],
                  dall.nightly.new[,":="(channel='nightly')]
                  )[,":="(v=NULL)]
dall.new <- dall.new[, list(date,os,usage_all,dau_all,usage_cversion,dau_cversion,dau_cm_crasher_cversion,dau_cc_crasher_cversion,
                            dau_cp_crasher_cversion,dau_call_crasher_cversion,usage_cm_crasher_cversion,
                            usage_cc_crasher_cversion,usage_cp_crasher_cversion,usage_call_crasher_cversion,
                            cmain,ccontent,cplugin,call,c_version,c_version_rel,isLatest,t,cmi,cmr,cci,ccr,nvc,major,minor,channel)]

```



```{r writeData,eval=get.dataq}

## What versions, channels, and os did we read today?
unq <- dall.new[, list(n=.N),by=list(major, channel)]

## delete them from BQ table
res <- unq[,{
  del.q <- glue("delete  from analysis.sguha_crash_rate_raw  where major={major} and channel='{channel}'",major=.BY$major, channel=.BY$channel)
  g$q(del.q)
},by=list(major,channel)]

## Upload new data
atemp <- tempfile()
write.csv(dall.new, file=atemp,row.names=FALSE)
system(glue("bq load --noreplace --project_id moz-fx-data-derived-datasets --source_format=CSV --skip_leading_rows=1 --null_marker=NA",
            " analysis.sguha_crash_rate_raw {atemp}"))

## Created FIRST TABLE via
#system(glue("bq load  --project_id moz-fx-data-derived-datasets --autodetect --replace  --source_format=CSV --skip_leading_rows=1 --null_marker=NA",
#            " analysis.sguha_crash_rate_raw {atemp}"))
```





## GET MODEL DATA




Current and last two versions:

```{r}
dall.rel2 <- g$q(glue("select * from analysis.sguha_crash_rate_raw where channel = 'release' and major in ({whichv})",
         whichv = paste(unique(release.releases.for.model[,major]),collapse=",")),-1)
dall.rel2 <- dall.rel2[nvc > 0][,":="(cmi.logit=boot::logit(cmi),cci.logit=boot::logit(cci),nvc.logit=boot::logit(nvc),cmr.wt = 1*(cmain >0), ccr.wt = 1*(ccontent > 0))]

dall.beta2 <- g$q(glue("select * from analysis.sguha_crash_rate_raw where channel = 'beta' and major in ({whichv})",
         whichv = paste(unique(beta.releases.for.model[,major]),collapse=",")),-1)
dall.beta2 <- dall.beta2[nvc > 0][,":="(cmi.logit=boot::logit(cmi),cci.logit=boot::logit(cci),nvc.logit=boot::logit(nvc),cmr.wt = 1*(cmain >0), ccr.wt = 1*(ccontent > 0))]

dall.nightly2 <- g$q(glue("select * from analysis.sguha_crash_rate_raw where channel = 'nightly' and major in ({whichv})",
                          whichv = paste(unique(nightly.releases.for.model),collapse=',')),-1)

dall.nightly2 <- dall.nightly2[nvc > 0][,":="(cmi.logit=boot::logit(cmi),cci.logit=boot::logit(cci),nvc.logit=boot::logit(nvc),cmr.wt = 1*(cmain >0), ccr.wt = 1*(ccontent > 0))]


#dall.rel2[, wts := 1+dau_cversion/max(dau_cversion)*100,by=list(os)]
#dall.beta2[, wts :=  1+dau_cversion/max(dau_cversion)*100,by=list(os)]
dall.rel2[, wts := 1]
dall.beta2[, wts:=1]
dall.nightly2[,wts := 1]
```


## BUILD MODELS

```{r}

ffunc <- function(M,D)  brm(M,data=D, chains = 4, control = list(adapt_delta = 0.999, max_treedepth=12), cores = 4)
      # M0 <- bf( log(1+cmain) | weights(cmr.wt)   ~  os + os*poly(log(usage_cm_crasher_cversion+1/60),2)+os*poly(nvc.logit,2)  + (1+os|c_version), sigma ~   os + s(nvc.logit, m=1))
make.a.model <- function(data,wh,channel='not-nightly'){
  ## See wbeards work on nightly: https://metrics.mozilla.com/protected/wbeard/mc/nightly_model.html
  alter <- TRUE
  if(wh=="cmr"){
      M0 <- bf( cmain+1 |weights(cmr.wt*wts)   ~  os+offset(log( usage_cm_crasher_cversion+1/60))  + s(nvc,m=1,by=os)+(1+os|c_version), shape ~ os+s(nvc,m=1))+negbinomial()
      if(channel=='nightly'){
        M0 <- bf( cmain + 1 |weights(cmr.wt*wts) ~ offset(log(usage_cm_crasher_cversion + 1/60)) + os + (1+os | c_version) + s(nvc, m = 1) , shape ~ log(dau_cversion + 1) + os)+negbinomial()
      }
  }
  if(wh=='ccr'){
    M0 <- bf( ccontent+1 | weights(ccr.wt*wts)   ~  os+offset(log( usage_cc_crasher_cversion+1/60))  + s(nvc,m=1,by=os) + (1+os|c_version), shape ~  os+s(nvc,m=1))+negbinomial()
    if(channel=='nightly'){
     M0 <- bf( ccontent + 1 | weights(ccr.wt*wts)~ os + offset(log(usage_cc_crasher_cversion + 1/60)) +  s(nvc, m = 1, by = os) + (1 + os | c_version),
               shape ~ os + s(nvc, m = 1))+negbinomial()
    }
  }
  if(wh=='cmi'){
    M0<- bf( log(1+dau_cm_crasher_cversion)|weights(wts)   ~   os+ offset(log( dau_cversion)) + s(nvc,m=1,by=os) + (1+os|c_version), sigma ~ os+s(nvc,m=1))
    if(channel=='nightly'){
      M0 <- bf(log(1 + dau_cm_crasher_cversion)|weights(wts) ~ os + offset(log(dau_cversion)) + s(nvc, m = 1) + (1 + os | c_version) ,sigma ~ os + nvc)
    }
  }
  if(wh=='cci'){
    M0<- bf( log(1+dau_cc_crasher_cversion)|weights(wts)   ~   os+ offset(log( dau_cversion))  + s(nvc,m=1,by=os) + (1+os|c_version), sigma ~ os+s(nvc,m=1))
    if(channel=='nightly'){
      M0 <- bf( log(1 + dau_cc_crasher_cversion) |weights(wts) ~ os + offset(log(dau_cversion)) + s(nvc, m = 1) + (1 + os | c_version),sigma ~ os + nvc)
    }
  }
  ffunc(M0,data)
}

Predict <- function(M,...){
  p <- predict(M,...)
  fa <- family(M)$fam
  if(fa=="gaussian") return(p)
  if(fa=="negbinomial") {return(log(p))}
}
    
  
getPredictions <- function(M,D, wh,givenx=NULL,COUNT=0){
  if(is.null(givenx)){
    x <-Predict(M,newdata=D,summary=FALSE)
  }else{
    x <-givenx
  }
  if(COUNT==1){
    return (exp(t(x)-1))
  }else if( COUNT==2){
    return(t(x))
  }
  if(wh=='cmr'){
    r <- exp( t(x) - D[, log( usage_cm_crasher_cversion+1/60)])
  }
  if(wh=='ccr'){
    r <- exp(t(x) -  D[, log( usage_cc_crasher_cversion+1/60)])
  }
  if(wh=='cmi'){
    r <- exp(t(x) -  D[, log( dau_cversion)])
    r[r>1]  <- 1
    #r <- boot::inv.logit(t(x))
    }
  if(wh=='cci'){
    r <- exp(t(x) -  D[, log( dau_cversion)])
    r[r>1] <- 1
    #r <- boot::inv.logit(t(x))
  }
  r
}
 
```

Release model

```{r releaseModel}
slack.start("cryptojoy2")
slackr("RELEASE CHAINS STARTING")
d.rel <- dall.rel2
cr.cm.rel.f <- future({ make.a.model(d.rel,'cmr') })
cr.cc.rel.f <- future({ make.a.model(d.rel,'ccr') })
ci.cm.rel.f <- future({ make.a.model(d.rel,'cmi') })
ci.cc.rel.f <- future({ make.a.model(d.rel,'cci') })

cr.cm.rel <- value(cr.cm.rel.f)
cr.cc.rel <- value(cr.cc.rel.f)
ci.cm.rel <- value(ci.cm.rel.f)
ci.cc.rel <- value(ci.cc.rel.f)
slackr("RELEASE CHAINS DONE")
```

Beta Model

```{r betaModel}
slackr("BETA CHAINS STARTING")
d.beta <- dall.beta2
cr.cm.beta.f <- future({ make.a.model(d.beta,'cmr') })
cr.cc.beta.f <- future({ make.a.model(d.beta,'ccr') })
ci.cm.beta.f <- future({ make.a.model(d.beta,'cmi') })
ci.cc.beta.f <- future({ make.a.model(d.beta,'cci') })
cr.cm.beta <- value(cr.cm.beta.f)
cr.cc.beta <- value(cr.cc.beta.f)
ci.cm.beta <- value(ci.cm.beta.f)
ci.cc.beta <- value(ci.cc.beta.f)
slackr("BETA CHAINS DONE!")
```



Nightly Model

```{r nightylModel}
slackr("Nightly CHAINS STARTING")
d.nightly <- dall.nightly2
cr.cm.nightly.f <- future({ make.a.model(d.nightly,'cmr',channel='nightly') })
cr.cc.nightly.f <- future({ make.a.model(d.nightly,'ccr',channel='nightly') })
ci.cm.nightly.f <- future({ make.a.model(d.nightly,'cmi',channel='nightly') })
ci.cc.nightly.f <- future({ make.a.model(d.nightly,'cci',channel='nightly') })
cr.cm.nightly <- value(cr.cm.nightly.f)
cr.cc.nightly <- value(cr.cc.nightly.f)
ci.cm.nightly <- value(ci.cm.nightly.f)
ci.cc.nightly <- value(ci.cc.nightly.f)
slackr("Nightly CHAINS DONE!")
```


I use a 1% trimmed mean because some of the posteriors especially when nvc is
small have crazy large outliers.

```{r}
TR <- 1/100/2
CENT <- function(s)  mean(s, trim=TR)
getLowsAndHighs <- function(D,mdl.rm,mdl.rc,mdl.im,mdl.ic){
  getLowHi <- function(x,names,trans){
    x <- rbindlist(apply(x,1,function(s){
      data.table(lo90 = trans(quantile(s,0.05)), mean = CENT(trans(s)), hi90=quantile(trans(s),0.95))
    }))
    setnames(x,names)
  }
  pm <- getPredictions(mdl.rm, D, 'cmr')
  pc <- getPredictions(mdl.rc, D, 'ccr')
  cmrpreds <- getLowHi(pm,trans=function(s) s,names=c("cmr.lo90","cmr.mean","cmr.hi90"))
  ccrpreds <- getLowHi(pc,trans=function(s) s,names=c("ccr.lo90","ccr.mean","ccr.hi90"))
  cavgrpreds <- getLowHi((pm+pc)/2,trans=function(s) s,names=c("cavgr.lo90","cavgr.mean","cavgr.hi90"))
  pm <- getPredictions(mdl.im, D, 'cmi')
  pc <- getPredictions(mdl.ic, D, 'cci')
  cmipreds <- getLowHi(pm,trans=function(s) s,names=c("cmi.lo90","cmi.mean","cmi.hi90"))
  ccipreds <- getLowHi(pc,trans=function(s) s,names=c("cci.lo90","cci.mean","cci.hi90"))
  cavgipreds <- getLowHi((pm+pc)/2,trans=function(s) s,names=c("cavgi.lo90","cavgi.mean","cavgi.hi90"))
  cbind(cmrpreds,ccrpreds,cavgrpreds, cmipreds, ccipreds, cavgipreds)
}

getRelDiff <- function(crm.model,crc.model,cim.model,cic.model,current.data,prev.version,CUT=0.20,CUT.I=0.15,avg=FALSE){
  getDiff <- function(current.data,whi='cmr',model,prev.version,aCUT,avg=FALSE){
    if(length(whi)==2){
      ## Averages
      cm.preds1 <- getPredictions(NULL, current.data,whi[1],given=Predict(model$m,summary=FALSE, newdata=current.data))
      cc.preds1 <- getPredictions(NULL, current.data,whi[2],given=Predict(model$c,summary=FALSE, newdata=current.data))
      cm.preds <- (cm.preds1  + cc.preds1)/2
    }else{
      ## Singles
      cm.preds <- getPredictions(NULL, current.data,whi,given=Predict(model,summary=FALSE, newdata=current.data))
    }
    cm.preds <- cbind(current.data[, list(os, c_version, nvc)],cm.preds)
    f <- cm.preds[,{
      that <- .SD[c_version==prev.version,3:ncol(.SD),with=FALSE]
      this <- .SD[c_version!=prev.version,3:ncol(.SD),with=FALSE]
      hx <- as.numeric((this-that)/that)
      hx <- hx[abs(hx)!=Inf & !is.na(hx)]
      if(!avg){
        h <-  mean(  hx > aCUT)
        V <- if(is.na(h)) 0 else h
      }else{
        h <- mean( hx ,trim=0.5/100/2)
        h <- median(hx)
        V <- if(is.na(h) || abs(h)==Inf) 0 else h
      }
      list(V=if(is.na(h)) 0 else h)#,W = mean(  (this-that)/that*100,trim=0.5/100/2) )
    },by=list(os,nvc)]
    f
  }
  cmr <- getDiff(current.data, whi="cmr", crm.model, prev.version=prev.version,aCUT=CUT,avg=avg)[, cmr.reg.prob := V][,V := NULL]
  ccr <- getDiff(current.data, whi='ccr', crc.model, prev.version=prev.version,aCUT=CUT,avg=avg)[, ccr.reg.prob := V][,V := NULL]
  cavgr <- getDiff(current.data, whi=c('cmr','ccr'), model=list(m=crm.model,c=crc.model),
                   prev.version=prev.version,aCUT=CUT,avg=avg)[, cavgr.reg.prob:=V][,V:=NULL]

  cmi <- getDiff(current.data, whi="cmi", cim.model, prev.version=prev.version,aCUT=CUT.I,avg=avg)[, cmi.reg.prob := V][,V := NULL]
  cci <- getDiff(current.data, whi="cci", cic.model, prev.version=prev.version,aCUT=CUT.I,avg=avg)[, cci.reg.prob := V][,V := NULL]
  cavgi <- getDiff(current.data, whi=c('cmi','cci'), model=list(m=crm.model,c=cim.model),prev.version=prev.version,aCUT=CUT.I,avg=avg)[, cavgi.reg.prob:=V][,V:=NULL]
  Reduce(function(x,y) merge(x,y,by=c("os","nvc")),list(current.data[c_version!=prev.version,list(os,date,c_version,nvc)],
                                                        cmr,ccr,cavgr,cmi,cci,cavgi))
}


```

```{r}

compare.two <- function(model,whi,this,that,CUT=0.35){
    getDiff <- function(current.data,whi,model){
      cm.preds <- getPredictions(NULL, current.data,whi,given=Predict(model,summary=FALSE, newdata=current.data))
      cm.preds <- cbind(current.data[, list(date,nvc,branch)],cm.preds)
      cm.preds <- cm.preds[, .SD[nvc==max(nvc),],by=branch]
      cm.preds$date <- NULL; cm.preds$nvc <- NULL
      cm.preds <- cm.preds[,list(x = as.numeric(.SD[,2:ncol(.SD)])), by=branch]
      plot1 <- ggplot(cm.preds, aes(x,color=branch))+geom_density()+ggtitle(glue("Density of two branches for {whi}"))
      plot2 <- ggplot(cm.preds, aes(x,color=branch))+geom_density()+scale_x_continuous(trans='log2') + ggtitle(glue("(Logged) Density of two branches for {whi}"))
      library(patchwork)
      plots  <-  plot1
      reldiff <- (cm.preds[branch=='this', x] - cm.preds[branch=='that',x])/cm.preds[branch=='that',x]*100
      list(plots=plots, means=cm.preds[abs(x)<Inf,][,mean(x), by=branch] , reldiff=list(i=quantile(reldiff, c(0.05,0.95)),q=quantile(reldiff, c(0.01,0.25,0.5,0.75,0.99)))
         , probReg = mean(reldiff >= (100*CUT))*100,mean = mean(reldiff[abs(reldiff)<Inf & !is.na(reldiff)], trim=0.5/100/2))
    }
    current.data=rbind(this[,"branch" := "this"], that[,"branch" := "that"])
    getDiff(current.data, whi, model)
}    

### Example Usage
##this <- d.nightly[c_version=='20190909' & os=='Linux',]
##that <- copy(this)[,c_version := '20190908']
##compa <- compare.two(model=cr.cc.nightly,whi='ccr', this,that,CUT=0.25)


## this <- d.rel[c_version=='69.0' & os=='Linux',]
## that <- copy(this)[,c_version := '68.0.2']
## compa <- compare.two(model=ci.cm.rel,whi='cmr', this,that)



```




For Release data structures


```{r releaseDataStructure}
release.releases.for.model <- release.releases.for.model[order(major,minor),]
rel.current.version <- tail(release.releases.for.model,2)[2,version]
rel.prev.version <- tail(release.releases.for.model,2)[1,version]
rel.current.data <- d.rel[c_version==rel.current.version,][order(os, date),]
rel.current.data.modified <- data.table(data.frame(rel.current.data))[, c_version := rel.prev.version]

lowhigh.rel <- getLowsAndHighs(d.rel,cr.cm.rel,cr.cc.rel,ci.cm.rel,ci.cc.rel)
reldiff.rel <- getRelDiff(cr.cm.rel,cr.cc.rel,ci.cm.rel,ci.cc.rel,
                          current.data=rbind(rel.current.data,rel.current.data.modified),
                          prev.version=rel.prev.version)


rel.posts <- cbind(d.rel[,list(date,cmr.wt,ccr.wt,c_version, os,nvc,cmr,ccr,cmi,cci)],lowhigh.rel)
rel.posts <- merge(rel.posts,reldiff.rel,all.x=TRUE,by=c("os","date","nvc","c_version"))
rel.summary <- rel.posts[c_version==rel.current.version,][date==max(date),][, os:=factor(os, levels=c("Windows_NT","Darwin","Linux"))]

H0 <- function(nvc,l,h){
  return( nvc > 0.2)
}
SPAN_L <- 2
rel.posts <- merge(rel.posts,rel.posts[,{
  i <- 1:.N
  cavgr <- (cmr+ccr)/2; cavgi <- (cmi+cci)/2
  cmr.fit <- loess(cmr.mean  ~ i,span=SPAN_L,weights=H0(nvc,cmr.lo90,cmr.hi90))$fitted
  ccr.fit <- loess(ccr.mean ~ i, span=SPAN_L, weights=H0(nvc,ccr.lo90,ccr.hi90))$fitted
  cavgr.fit <- loess(cavgr.mean ~ i, span=SPAN_L, weights=H0(nvc,cavgr.lo90,cavgr.hi90))$fitted
  cmi.fit <- loess(cmi.mean  ~ i, span=SPAN_L, weights=H0(nvc,cmi.lo90,cmi.hi90))$fitted
  cci.fit <- loess(cci.mean ~ i, span=SPAN_L, weights=H0(nvc,cci.lo90,cci.hi90))$fitted
  cavgi.fit <- loess(cavgi.mean ~ i, span=SPAN_L, weights=H0(nvc,cavgi.lo90,cavgi.hi90))$fitted
  list(date=date,c_version=c_version,cmr.loess=cmr.fit,
       ccr.loess=ccr.fit, cavgr.loess=cavgr.fit,
       cmi.loess=cmi.fit, cci.loess=cci.fit, cavgi.loess=cavgi.fit)
  }, by=os], by=c("os","date","c_version"))

```

For beta data structures

```{r betaDataStructure}
beta.releases.for.model <- beta.releases.for.model[order(major,minor),]
beta.current.version <- tail(beta.releases.for.model,2)[2,version] 
beta.prev.version <-  tail(beta.releases.for.model,2)[1,version]
beta.current.major <- tail(beta.releases.for.model,2)[2,major]
beta.prev.major <- beta.current.major-1
beta.current.data <- d.beta[c_version==beta.current.version,][order(os, date),]
beta.current.data.modified <- data.table(data.frame(beta.current.data))[, c_version := beta.prev.version]

lowhigh.beta <- getLowsAndHighs(d.beta,cr.cm.beta,cr.cc.beta,ci.cm.beta,ci.cc.beta)
reldiff.beta <- getRelDiff(cr.cm.beta,cr.cc.beta,ci.cm.beta,ci.cc.beta,
                           current.data=rbind(beta.current.data,beta.current.data.modified),
                           prev.version = beta.prev.version)
beta.posts <- cbind(d.beta[,list(date,cmr.wt,ccr.wt,c_version, os,nvc,cmr,ccr,cmi,cci)],lowhigh.beta)
beta.posts <- merge(beta.posts,reldiff.beta,all.x=TRUE,by=c("os","date","nvc","c_version"))

beta.summary <- beta.posts[c_version==beta.current.version,][,tail(.SD[order(date),],1),by=os][, os:=factor(os, levels=c("Windows_NT","Darwin","Linux"))]
beta.trend2 <- beta.posts[, tail(.SD[order(nvc),],1),by=list(os,c_version)]
beta.trend2[, ":="(major =  (sapply(c_version,function(s) head(strsplit(s,"\\.0b")[[1]],1))),
                   minor = (sapply(c_version,function(s) tail(strsplit(s,"\\.0b")[[1]],1))))]
beta.trend2 <- beta.trend2[major %in% c(beta.current.major,beta.prev.major),]



#beta.trend3 <- getBetaTrend(d.beta,cr.cm.beta,cr.cc.beta,ci.cm.beta,ci.cc.beta)
#beta.trend3[, ":="(major =  (sapply(c_version,function(s) head(strsplit(s,"\\.0b")[[1]],1))),
#                  minor = (sapply(c_version,function(s) tail(strsplit(s,"\\.0b")[[1]],1))))]
#beta.trend2 <- beta.trend3[major %in% c(beta.current.major,beta.prev.major),][order(os,as.numeric(as.character(major)),as.numeric(as.character(minor))),]
#beta.trend2 <- merge(beta.trend2,  beta.posts[order(c_version,nvc),][, {
# tail(.SD,1)[, list(nvc=nvc)]
#},by=list(os,c_version)], by=c("os","c_version"))[order(os,as.numeric(as.character(major)),as.numeric(as.character(minor))),]

```


```{r nightlyDatastructure}
nightly.this.major <- max(d.nightly$major)

nightly.current.version <- max(d.nightly$c_version)
nightly.prev.version <-  rev(sort(unique(d.nightly$c_version)))[2]
nightly.current.data <- d.nightly[c_version==nightly.current.version,][order(os, date),]
nightly.current.data.modified <- data.table(data.frame(nightly.current.data))[, c_version := nightly.prev.version]


lowhigh.nightly <- getLowsAndHighs(d.nightly,cr.cm.nightly,cr.cc.nightly,ci.cm.nightly,ci.cc.nightly)
reldiff.nightly <- getRelDiff(cr.cm.nightly,cr.cc.nightly,ci.cm.nightly,ci.cc.nightly,
                           current.data=rbind(nightly.current.data,nightly.current.data.modified),
                           prev.version = nightly.prev.version,avg=TRUE)
nightly.posts <- cbind(d.nightly[,list(major,minor,date,cmr.wt,ccr.wt,c_version, os,nvc,cmr,ccr,cmi,cci)],lowhigh.nightly)
nightly.posts <- merge(nightly.posts,reldiff.nightly,all.x=TRUE,by=c("os","date","nvc","c_version"))

nightly.summary <- nightly.posts[c_version==nightly.current.version,][, tail(.SD[order(date),],1)   ,by=os][, os:=factor(os, levels=c("Windows_NT","Darwin","Linux"))]
nightly.trend2 <- nightly.posts[, tail(.SD[order(nvc),],1),by=list(os,c_version)]

```

This is the important bit for consistency. The model takes all the data and fits
the daily values. Obviously this wil change in presence of new data. Thus the
values for 2019-09-05 will look different for the data in common with the report
for 2019-09-06. This can be confusing to readers. So we will not change data for
past days. Instead we will merge latest data (date, nvc,os) which is no present
in older data.


```{r}

## SAVE
```{r eval=TRUE}

jdate <- Sys.Date()
zq <- expression({
  u <- getwd()
  setwd("visual_demo/data/")
  write.csv(rel.posts, "rel.posts.csv")
  write.csv(rel.summary,"rel.summary.csv")
  write.csv(beta.posts,"beta.posts.csv")
  write.csv(beta.summary,"beta.summary.csv")
  write.csv(beta.trend2, "beta.trend.csv")
  setwd(u);rm(u)

  ### I put older ones into BQ alongwith generate date
  ## i ignored the current one for testing purposes
  ## technically wrong since i upload past model , current model for new data
  ## and here i'm  uploading older models ...
  ## To backfill choose a seed model and then new models will just append data
  #  print(s);
  ## ay=fread(glue("./visual_demo/older/{w}/data/rel.posts.csv",w=s))[,V1 := NULL]
  ## ay[is.na(cmr.reg.prob), cmr.reg.prob := 0]
  ## ay[is.na(ccr.reg.prob), ccr.reg.prob := 0]
  ## ay[is.na(cavgr.reg.prob), cavgr.reg.prob := 0]
  ## ay[is.na(cmi.reg.prob), cmi.reg.prob := 0]
  ## ay[is.na(cci.reg.prob), cci.reg.prob := 0]
  ## ay[is.na(cavgi.reg.prob), cavgi.reg.prob := 0]

  ## ay[, "genDate" := s]
  ## write.csv(ay,"/tmp/u.csv",row.names=FALSE)
  ## system(glue("bq load  --project_id moz-fx-data-derived-datasets --autodetect --replace  --source_format=CSV --skip_leading_rows=1 --null_marker=NA",
  ##               " analysis.sguha_crash_rate_release_posts /tmp/u.csv"))
  ## #}

  ## ay=fread(glue("./visual_demo/older/{w}/data/beta.posts.csv",w=s))[,V1 := NULL]
  ## ay[is.na(cmr.reg.prob), cmr.reg.prob := 0]
  ## ay[is.na(ccr.reg.prob), ccr.reg.prob := 0]
  ## ay[is.na(cavgr.reg.prob), cavgr.reg.prob := 0]
  ## ay[is.na(cmi.reg.prob), cmi.reg.prob := 0]
  ## ay[is.na(cci.reg.prob), cci.reg.prob := 0]
  ## ay[is.na(cavgi.reg.prob), cavgi.reg.prob := 0]
  ## ay[, "genDate" := s]
  ## write.csv(ay,"/tmp/u.csv",row.names=FALSE)
  ## system(glue("bq load  --project_id moz-fx-data-derived-datasets --autodetect  --replace  --source_format=CSV --skip_leading_rows=1 --null_marker=NA",
  ##             " analysis.sguha_crash_rate_beta_posts /tmp/u.csv"))
  
  ## The posts file has a seed model
  ## and every day this is run the fitted data for the new day(s) is added

  system(glue("rm -rf  ./visual_demo/older/{today}/", today=jdate))
  
  Fixup <- function(older, newer){
    ## older tables(in BQ) have _ instead of . !!!
    ## need to fix this
    o <- colnames(older); o <- gsub("_",".",o,perl=FALSE);o[o=='c.version']='c_version';setnames(older,o)
    az1 <- merge(older[, list(use=1),by=list(os,date)], newer[, list(use=1),by=list(os,date)],all.y=TRUE, by=c("os","date"))
    f <- list(older=older,newstuff=merge(az1[is.na(use.x),list(os,date)], newer))
    f$result=rbind(f$older,f$newstuff)
    f
  }
  
  rel.posts.0 <- Fixup(g$q(glue("select * except(genDate) from analysis.sguha_crash_rate_release_posts"),-1), rel.posts)
  rel.posts.1 <- rel.posts.0$result[order(date),]
  rel.summary.1 <- rel.summary

  beta.posts.0 <- Fixup(g$q(glue("select * except(genDate) from analysis.sguha_crash_rate_beta_posts"),-1), beta.posts)
  beta.posts.1 <- beta.posts.0$result[order(date),]
  beta.summary.1 <- beta.summary #


  nightly.posts.1 <- nightly.posts
  nightly.summary.1 <- nightly.summary
  
  ## Now upload newest data
  UploadPost <- function(d,channel,j){
    if(nrow(d) > 0){
      d <- d[, genDate := j]
      write.csv(d, "/tmp/u.csv",row.names=FALSE)
      system(glue("bq load  --project_id moz-fx-data-derived-datasets    --source_format=CSV --skip_leading_rows=1 --null_marker=NA",
                  " analysis.sguha_crash_rate_{channel}_posts /tmp/u.csv"))
      }
  }

  ## upload the fixedup posts 
  UploadPost(rel.posts.0$newstuff,"release",jdate) 
  UploadPost(beta.posts.0$newstuff,"beta",jdate)
  
  clean_site("./visual_demo")
  render_site("./visual_demo/")

SKEL <- '
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title></title>
  <meta name="author" content="">
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/normalize.css" rel="stylesheet">
  <link href="css/style.css" rel="stylesheet">
</head>
<body>
<ul>
{body}
</ul>
</body>
</html>
'

datum<- glue(SKEL, body=paste(unlist(lapply(system("ls ./visual_demo/_site/data/",intern=TRUE),function(s){
  if(s!="index.html") glue("<li><a href='./{foo}'>{foo}</a></li>", foo=s)
})),collapse="\n"))
writeLines(datum,'./visual_demo/_site/data/index.html')

system(glue("mkdir -p ./visual_demo/older/{today}/", today=jdate))
system(glue("rsync -avz ./visual_demo/_site/ ./visual_demo/older/{today}/",today=jdate))
system("rsync -av ~/mz/missioncontrol/ex1/visual_demo/ ~/pubsguha/visual_demo/")

older <- glue(SKEL, body=paste(unlist(lapply(system("ls ~/pubsguha/visual_demo/older/",intern=TRUE),function(s){
  if(s!="index.html") glue("<li><a href='./{foo}'>{foo}</a></li>", foo=s)
})),collapse="\n"))
writeLines(older,'~/pubsguha/visual_demo/older/index.html')

  
})


```

```{r eval=FALSE}

gen <- expression({
  alter=TRUE;render("report.3.Rmd");
  eval(zq)
})

testing <- expression({
  alter <- TRUE;render("report.3.Rmd")
  clean_site("./visual_demo")
  render_site("./visual_demo/")
})


```

## PLOTS

### Crash Rate

```{r pp3}

pp_check(cr.cc.rel, type='stat_grouped', group='os')
pp_check(cr.cm.rel, type='stat_grouped', group='os')
pp_check(cr.cm.beta, type='stat_grouped', group='os')
pp_check(cr.cc.beta, type='stat_grouped', group='os')
```


### Crash Incidence

```{r pp2}
pp_check(ci.cm.rel, type='stat_grouped', group='os')
pp_check(ci.cc.rel, type='stat_grouped', group='os')
pp_check(ci.cm.beta, type='stat_grouped', group='os') 
pp_check(ci.cc.beta, type='stat_grouped', group='os')
```

### Correlations

```{r}

rel.posts[!is.na(cmr) & cmr !=Inf, cor(cmr,cmr.mean),by=os]
rel.posts[!is.na(ccr) & ccr !=Inf ,cor(ccr,ccr.mean),by=os]
beta.posts[!is.na(cmr) & cmr !=Inf,cor(cmr,cmr.mean),by=os]
beta.posts[!is.na(ccr) & ccr !=Inf,cor(ccr,ccr.mean),by=os]


rel.posts[,cor(cmi,cmi.mean),by=os]
rel.posts[,cor(cci,cci.mean),by=os]
beta.posts[,cor(cmi,cmi.mean),by=os]
beta.posts[,cor(cci,cci.mean),by=os]

```
